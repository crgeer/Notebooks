{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crgeer/Notebooks/blob/main/Audio_Adv_ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WABnpOwfael3"
      },
      "source": [
        "Welcome to the Audio Adversarial Machine Learning Project!\n",
        "\n",
        "In this project, you will receive a passive introduction to machine learning in the audio domain by reading, comprehending, and running ML code on audio datasets.  You will also implement the Basic Iterative Method (BIM) attack and the Density Estimation anomaly detection defense.  The project assumes this is not your first time using PyTorch, although if it is you may still be able to slog through.\n",
        "\n",
        "This project is intended for Google Colab.  Google Colab is a free resource that runs this Jupyter notebook in a virtual machine with optional hardware acceleration (Runtime -> Change runtime type -> GPU).  But BEWARE - as a free resource, there are NO guarantees for length of time you can access a VM or the resources that are available to you.  What you ran yesterday may not run today.  Also, you might get a snack while your model is training and come back to find your session erased.  Be aware up front that Colab can be tricky, especially in the free version.  There are options to pay for an improved Colab environment (I did not), or pay for increased storage so you can save checkpoints (I did).  The project has been verified to run in a totally free setting, using less than 5 GB of Google Drive storage for the original dataset.  You can also take this Jupyter notebook and work offline, particularly if you can run this code on your own GPU.\n",
        "\n",
        "Now let's get started.  Run the following cell by using Shift + Enter to import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS87LvTyZ5tN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa, librosa.display, os, pickle, random, torch, torchaudio, torchvision\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.mixture import GaussianMixture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgF7kPlwF428"
      },
      "source": [
        "Now take the files at this link (https://drive.google.com/drive/folders/1h6FTQD4G0BlzOuGdgLsjn6NJOnM9KYuF?usp=sharing) and upload them to your Google Drive account.  After the folder is in your Google Drive, run the following code block to import your Google Drive contents into this session.  This will allow you to access the Audio MNIST dataset.\n",
        "\n",
        "The Audio MNIST dataset (https://github.com/soerenab/AudioMNIST) contains WAV recordings of 60 speakers saying the digits 0 to 9.  Each speaker says each digit 50 times - meaning there are 30,000 short audio clips in this dataset.  We will use this throughout the project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OXw9tVzacym"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v84uzPvpOoP-"
      },
      "source": [
        "Take a look at the code below.  Some of the lines are *IMPORTANT* to check, like matching up the data location variable to the location in your Colab filesystem where you have the dataset.\n",
        "\n",
        "While the functional lines work as-is and shouldn't require any tweaking, I recommend reading them for comprehension so that you can follow the processes here.  Some notes:\n",
        "\n",
        "STFT - Short time Fourier transform - this decomposes an audio signal into the frequencies that it comprises over time.  The decomposition takes a small time segment of the audio signal and generates complex-value coefficients for those component frequencies.  With a vector of coefficients for each time segment, and many time segments, the result of the STFT operation is a 2-D matrix.  The dimensions and values of that matrix can vary by tweaking the parameters to the STFT operation, but the arrangement below will work well for you.\n",
        "\n",
        "Spectrogram - This term will pop up throughout the project.  It is the 2-D matrix output from the STFT operation, just processed a little more so that it becomes both ingestible to a neural network and visually meaningful to a human.  We construct it by removing the complex phase information from the coefficients and converting values into decibels.\n",
        "\n",
        "Feel free to dive more into the audio-specific operations, but this project focuses primarily on adversarial AI attack and defense procedures.  So, also feel free to use those audio operations as black-box functions and keep moving.\n",
        "\n",
        "This will take around 30 minutes, so go grab a coffee.  Just try to click around the page now and then to make sure you don't time out your Colab session in the meantime.  Also, if you have the space, it would be really good to save this data so you don't have to re-run this cell.  Google currently offers 100GB of storage for a month for 2 dollars, and that 2 dollars might save you a lot of time.  But again, this project is intended to work entirely for free!  It just may require more patience that way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On4cBpaQFvA5"
      },
      "outputs": [],
      "source": [
        "print(\"Starting - \", datetime.now())\n",
        "\n",
        "# You might have to change this to fit your filesystem\n",
        "data_location = 'drive/MyDrive/Audio MNIST'\n",
        "\n",
        "# If you have some room (2 GB or so) to spare in your Google Drive, I suggest\n",
        "# saving the preprocessed spectrograms there.  This will make them\n",
        "# recoverable if your Colab session restarts.\n",
        "save_location = 'drive/MyDrive/Audio Adv ML Work Folder'\n",
        "\n",
        "# If you do not have room to spare, uncomment this line This probably will result\n",
        "# in you rerunning a lot of code, but that makes it easier to keep things free\n",
        "#save_location = 'temp'\n",
        "\n",
        "# Make space for saving data\n",
        "if not os.path.isdir(save_location):\n",
        "  os.mkdir(save_location)\n",
        "\n",
        "if not os.path.isdir(os.path.join(save_location, 'Preprocessed')):\n",
        "  os.mkdir(os.path.join(save_location, 'Preprocessed'))\n",
        "\n",
        "counter = 0\n",
        "\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "# The iterations follow the 60 speakers in the dataset\n",
        "for i in range(60):\n",
        "    \n",
        "  i += 1\n",
        "\n",
        "  # Formatting so we can parse the directory\n",
        "  if i < 10:\n",
        "    i = '0' + str(i)\n",
        "  else:\n",
        "    i = str(i)\n",
        "  \n",
        "  speaker_location = os.path.join(data_location, i)\n",
        "\n",
        "  # Now read the audio files\n",
        "  for audio_file in os.listdir(speaker_location):\n",
        "\n",
        "    nums = audio_file[5:7]\n",
        "\n",
        "    if nums[1] == '.':\n",
        "      nums = nums[0]\n",
        "    \n",
        "    nums = int(nums)\n",
        "\n",
        "    # Divide the data in an 80/10/10 split\n",
        "    # Adjust if you like\n",
        "    if nums < 40:\n",
        "      variant = 'Train'\n",
        "    elif nums < 45:\n",
        "      variant = 'Dev'\n",
        "    else:\n",
        "      variant = 'Test'\n",
        "    \n",
        "    # Comment out these lines if you have sufficient memory to access a\n",
        "    # larger training set.  At first, leave them  in to decrease the likelihood\n",
        "    # of crashing your VM due to insufficient memory\n",
        "    if nums > 20 and nums < 40:\n",
        "      continue\n",
        "    \n",
        "    # Load the base audio\n",
        "    audio, sample_rate = librosa.load(os.path.join(speaker_location, audio_file))\n",
        "    audio = librosa.util.fix_length(audio, 25000)\n",
        "    label = int(audio_file[0])\n",
        "\n",
        "    # Process into STFT spectrograms\n",
        "    stft = librosa.amplitude_to_db(abs(librosa.stft(audio, n_fft = 512)))\n",
        "    stft = transform(stft)\n",
        "\n",
        "    # Package up the data and store it\n",
        "    data = (stft, i, label)\n",
        "\n",
        "    if not os.path.isdir(os.path.join(save_location, 'Preprocessed', variant)):\n",
        "      os.mkdir(os.path.join(save_location, 'Preprocessed', variant))\n",
        "      \n",
        "    if not os.path.isdir(os.path.join(save_location, 'Preprocessed', variant, str(label))):\n",
        "      os.mkdir(os.path.join(save_location, 'Preprocessed', variant, str(label)))\n",
        "\n",
        "    pickle.dump(data, open(os.path.join(save_location, 'Preprocessed', variant, str(label), audio_file[0:7] + '.p'), 'wb'))\n",
        "\n",
        "    counter += 1\n",
        "\n",
        "print(\"Complete - \", datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzqZj-0btFaP"
      },
      "source": [
        "If your Colab session restarts, you'll need to run the cells from the top of the screen on down.  However, if you saved your preprocessed data to a persistent location, you can recover it by just aligning your file locations.  Run the cell below instead of the cell above in that case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNjs1EQwtX-E"
      },
      "outputs": [],
      "source": [
        "data_location = 'drive/MyDrive/Audio MNIST'\n",
        "save_location = 'drive/MyDrive/Audio Adv ML Work Folder'\n",
        "sample_rate = 22050 # if you used the provided dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj-oTPeOuNeI"
      },
      "source": [
        "If you still have your data open, let's open that last spectrogram to see what it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H6ouJwGuMkm"
      },
      "outputs": [],
      "source": [
        "stft = librosa.amplitude_to_db(abs(librosa.stft(audio, n_fft = 512)))\n",
        "spectrogram = librosa.display.specshow(stft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qadebYpQvsKk"
      },
      "source": [
        "And here's what it sounds like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JODhK9nvvfZ"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(data = audio, rate = sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JNj3t6YTlsN"
      },
      "source": [
        "There's still some work to do before we can feed data to a model.  Let's add a class here that will assist in loading the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-VbpIfiTbUC"
      },
      "outputs": [],
      "source": [
        "# This dataset class prepares the saved data so\n",
        "# that it can be ingested by a DataLoader\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "\n",
        "  def __init__(self, location):\n",
        "\n",
        "    self.path = location\n",
        "    self.data = {}\n",
        "    self.classes = []\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    # Iterate throught the 10 digits\n",
        "    for i in range(10):\n",
        "\n",
        "      # Iterate throught the files in each folder\n",
        "      for audio_file in os.listdir(os.path.join(self.path, str(i))):\n",
        "\n",
        "        audio, speaker, label = pickle.load(open(os.path.join(self.path, str(i), audio_file), 'rb'))\n",
        "        self.data[counter] = (audio, label)\n",
        "\n",
        "        if label not in self.classes:\n",
        "          self.classes.append(label)\n",
        "        \n",
        "        counter += 1\n",
        "  \n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    return self.data[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S96_02BuyQ6w"
      },
      "source": [
        "Finally, we can construct a model.  This model is loosely based on VGG-16 but is constructed to be adaptable to various input sizes and easy to adjust.  I recommend leaving it as-is for now, just because that way we can limit the complexity of the project to the places where it is intended.  Once you have things working, feel free to try new things!\n",
        "\n",
        "The model accepts a spectrogram as input and classifies it as one of the digits zero to nine.\n",
        "\n",
        "If the contents of the model are new to you, take a look at the PyTorch documentation for the different operations and layer types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crk1-2KjZvSo"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, dropout_param = 0):\n",
        "\n",
        "    # This is stored separately so that\n",
        "    # it can be manually adjusted\n",
        "    self.dropout_param = dropout_param\n",
        "\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 5, kernel_size = 7)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 5, out_channels = 5, kernel_size = 7, padding = 'same')\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "    self.bn1 = nn.BatchNorm2d(5)\n",
        "    self.drop = nn.Dropout(p = self.dropout_param)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 5, out_channels = 10, kernel_size = 5)\n",
        "    self.conv4 = nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, padding = 'same')\n",
        "    self.bn2 = nn.BatchNorm2d(10)\n",
        "    self.conv5 = nn.Conv2d(in_channels = 10, out_channels = 16, kernel_size = 5)\n",
        "    self.conv6 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 5, padding = 'same')\n",
        "    self.bn3 = nn.BatchNorm2d(32)\n",
        "    self.fc1 = nn.Linear(17920, 32)\n",
        "    self.fc2 = nn.Linear(32, 32)\n",
        "    self.fc3 = nn.Linear(32, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.bn1(self.pool(self.conv2(self.conv1(x))))\n",
        "    # This feedforward logic supports removing dropout manually\n",
        "    if self.dropout_param:\n",
        "      x = self.drop(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.bn2(self.pool(self.conv4(self.conv3(x))))\n",
        "    if self.dropout_param:\n",
        "      x = self.drop(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.bn3(self.pool(self.conv6(self.conv5(x))))\n",
        "    if self.dropout_param:\n",
        "      x = self.drop(x)\n",
        "    x = F.relu(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return F.log_softmax(x, dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtO-CwKOy-dV"
      },
      "source": [
        "We'll train the model now using the data you preprocessed earlier.  Fortunately, we are working with a large dataset.  This means we shouldn't run into much trouble with overfitting and can adopt a direct and straightforward training methodology.  Still, verify a few cells below with the test run to ensure test time performance is acceptable.  Re-run if necessary.  Take a look at the hyperparamers; you can tune these based on your needs.  However, the settings here will likely suffice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CPFmMNGX7dI"
      },
      "outputs": [],
      "source": [
        "print('Starting code block: ', datetime.now())\n",
        "\n",
        "# Hyperparameters and tweakable things\n",
        "epochs = 4\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "dropout_param = 0.4\n",
        "save_model = True # For me, this is only 3.2 MB\n",
        "model_name = 'CNN.pth'\n",
        "\n",
        "# Load up out saved data\n",
        "train_loc = os.path.join(save_location, 'Preprocessed', 'Train')\n",
        "train_set = AudioDataset(train_loc)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "# Construct your model\n",
        "net = CNN(dropout_param = dropout_param)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# This is a sanity check for if you are intending to use a GPU\n",
        "print('This network is running on device: ', device)\n",
        "\n",
        "# Know what device your model and data is on to help with debugging\n",
        "net.to(device)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr = lr)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  print(\"Starting epoch\", epoch + 1, \" - \", datetime.now())\n",
        "\n",
        "  for data in train_loader:\n",
        "\n",
        "    # Perform one step of training with one batch\n",
        "    audio, label = data\n",
        "    audio, label = audio.to(device), label.to(device)\n",
        "    net.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "    output = net(audio.view(-1, 1, 257, 196))\n",
        "    loss = F.nll_loss(output, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Tally up the performance\n",
        "    for idx, i in enumerate(output):\n",
        "\n",
        "      if torch.argmax(i) == label[idx]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "  counter += 1\n",
        "  \n",
        "  print(counter, '/', epochs, '  Loss: ', loss.item(), '  Accuracy: ', round(correct / total, 3))\n",
        "\n",
        "if save_model:\n",
        "  if not os.path.isdir(os.path.join(save_location, 'Models')):\n",
        "    os.mkdir(os.path.join(save_location, 'Models'))\n",
        "  \n",
        "  torch.save(net.state_dict(), os.path.join(save_location, 'Models', model_name))\n",
        "\n",
        "print('Code block finished: ', datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcrNa_QjuVYf"
      },
      "source": [
        "Again, if you saved your trained model parameters, you can recover them here instead of re-training the model if your Colab session restarts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftMCZpyndJYw"
      },
      "outputs": [],
      "source": [
        "model_name = 'CNN.pth'\n",
        "dev_loc = os.path.join(save_location, 'Preprocessed', 'Dev')\n",
        "batch_size = 128\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net = CNN().to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  net.load_state_dict(torch.load(os.path.join(save_location, 'Models', model_name), map_location = 'cuda:0'))\n",
        "  net.eval()\n",
        "  net.cuda()\n",
        "else:\n",
        "  net.load_state_dict(torch.load(os.path.join(save_location, 'Models', model_name), map_location = 'cpu'))\n",
        "  net.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgydTwSw0D8h"
      },
      "source": [
        "Let's see how the trained model performs on new data!\n",
        "\n",
        "You might notice that we constructed sets for Training, Development, and Testing.  For the basic version of the project, we only use the Training and Development sets.  This leaves the Test set available for you later if you want more data to validate choices for optional extensions to the project.  So here and later on, we actually use the Development set for \"testing.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPn2QWRNzyLj"
      },
      "outputs": [],
      "source": [
        "# Load up the Dev set\n",
        "dev_loc = os.path.join(save_location, 'Preprocessed', 'Dev')\n",
        "dev_loader = torch.utils.data.DataLoader(AudioDataset(dev_loc), batch_size = batch_size, shuffle = False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  \n",
        "  net.dropout_param = 0\n",
        "\n",
        "  for data in dev_loader:\n",
        "\n",
        "    # Only forward passes needed since we are just testing\n",
        "    audio, label = data\n",
        "    audio = audio.to(device)\n",
        "    label = label.to(device)\n",
        "    output = net(audio.view(-1, 1, 257, 196))\n",
        "    \n",
        "    for idx, i in enumerate(output):\n",
        "\n",
        "      if torch.argmax(i) == label[idx]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "  print(\"Accuracy: \", round(correct / total, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ww2M7c14FS"
      },
      "source": [
        "Here's the first step that is completely for you.\n",
        "\n",
        "You will implement the Basic Iterative Method.  The fundamental actions of the attack are very simple - but it can be difficult to pull concepts out of papers, skillfully use ML frameworks, and put everything together effectively.  Here's a paper that contains the concepts (https://arxiv.org/pdf/1607.02533.pdf).\n",
        "\n",
        "Note that the paper contains a couple variations of the central attack concept.  Some are untargeted and do not specify the target class, and some are targeted and do specify a specific target class.  The function you implement is provided one instance of a spectrogram, the model, and a target label.  You must construct the attack so that the model selects the target label with good effectiveness.  The output for this function should be a perturbed spectrogram of similar dimensions to the input spectrogram.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw5CUvoMHFIG"
      },
      "outputs": [],
      "source": [
        "def bim(data, net, target_label, epsilon = 1):\n",
        "\n",
        "  # Your code goes here\n",
        "\n",
        "  return data.view(257, 196)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y02skP635zO"
      },
      "source": [
        "This is the code that will test your attacks.  One thing that might be difficult for you is that your attack is converted back into pure audio before it is delivered to the victim model.  This more realistically simulates an attack on, say, an online speech-to-text model.  We are essentially doing just that, except for now the speech-to-text model only works for the digits zero to nine.  The conversion back to audio can be difficult because it might introduce approximation loss that could degrade the effects of a subtle adversarial perturbation.\n",
        "\n",
        "My naive implementation of BIM gets around 88% effectiveness.  Can you match that, or even do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8IaKtA3uWZa"
      },
      "outputs": [],
      "source": [
        "print('Code block started: ', datetime.now())\n",
        "\n",
        "# Reduce batch_size to 1 to simplify attack coding\n",
        "dev_loader = torch.utils.data.DataLoader(AudioDataset(dev_loc), batch_size = 1, shuffle = False)\n",
        "\n",
        "# This generates random target labels\n",
        "def get_target(source_label):\n",
        "\n",
        "  a = random.randint(0, 9)\n",
        "\n",
        "  while a == label[0]:\n",
        "    a = random.randint(0, 9)\n",
        "\n",
        "  return torch.tensor([a])\n",
        "\n",
        "\n",
        "attack_total = 0\n",
        "attack_succeed = 0\n",
        "\n",
        "successful_attacks = []\n",
        "\n",
        "for data, label in dev_loader:\n",
        "\n",
        "  data, label = data.to(device), label.to(device)\n",
        "  output = net(data.view(-1, 1, 257, 196))\n",
        "  data.requires_grad = True\n",
        "  l = label.item()\n",
        "  adv_label = get_target(label).to(device)\n",
        "  \n",
        "  # If the model is wrong at the start, we don't bother with an attack\n",
        "  if torch.argmax(output).item() != l:\n",
        "    continue\n",
        "  \n",
        "  attack_total += 1\n",
        "\n",
        "  # The call to your function\n",
        "  perturbed_data = bim(data, net, adv_label)\n",
        "\n",
        "  # This block simulates a more realistic attack delivery by reconstituting\n",
        "  # an audio signal, and then doing the pre-processing steps again to get\n",
        "  # a fresh spectrogram.  The simulation introduces minor approximation loss.\n",
        "  d_perturbed_data = perturbed_data.detach().cpu().numpy()\n",
        "  istft = librosa.db_to_amplitude(d_perturbed_data)\n",
        "  i_audio = librosa.util.fix_length(librosa.griffinlim(istft), 25000)\n",
        "  stft = librosa.amplitude_to_db(np.abs(librosa.stft(i_audio, n_fft = 512)))\n",
        "  stft = torch.from_numpy(stft).to(device)\n",
        "\n",
        "  output = net(stft.view(-1, 1, 257, 196))\n",
        "\n",
        "  # Let's see if the attack achieves the targeted label\n",
        "  if torch.argmax(output).item() == adv_label:\n",
        "    attack_succeed += 1\n",
        "    successful_attacks.append((d_perturbed_data, l, adv_label))\n",
        "  \n",
        "  # Shows progress - counts to about 3000 on the original code\n",
        "  if attack_total % 500 == 0:\n",
        "    print(attack_total, \"attacks attempted\")\n",
        "  \n",
        "if not os.path.isdir(os.path.join(save_location, 'Successful Attacks')):\n",
        "  os.mkdir(os.path.join(save_location, 'Successful Attacks'))\n",
        "\n",
        "# Comment this line out if you need to save storage\n",
        "torch.save(successful_attacks, open(os.path.join(save_location, 'Successful Attacks', 'bim.p'), 'wb'))\n",
        "\n",
        "print(\"BIM Success Rate: \", round(attack_succeed / attack_total, 3))\n",
        "\n",
        "print('Code block finished: ', datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkwY-aaFFR46"
      },
      "source": [
        "Here's another checkpoint load, if you need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv5fmcdgyPUW"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  successful_attacks = torch.load(os.path.join(save_location, 'Successful Attacks', 'bim.p'), map_location = 'cuda:0')\n",
        "else:\n",
        "  successful_attacks = torch.load(os.path.join(save_location, 'Successful Attacks', 'bim.p'), map_location = 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcq5j_E451_p"
      },
      "source": [
        "If you still have the data in your session, we can take a look at the spectrogram from the last attack you generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGjlGLXb58yf"
      },
      "outputs": [],
      "source": [
        "stft = librosa.amplitude_to_db(abs(librosa.stft(i_audio, n_fft = 512)))\n",
        "spectrogram = librosa.display.specshow(stft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNa7f-nh59D_"
      },
      "source": [
        "And produce the audio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT_9X5Kj5-0E"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(data = i_audio, rate = sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0jrUiWX5--z"
      },
      "source": [
        "You can probably hear some artifacts of the attack - hopefully not too much!  If things go well, you should only hear some faint static.  If the artifacts are immense, or if you destroyed the base audio signal entirely, don't worry too much.  Just check to see if you can improve your BIM implementation or maybe catch a bug or two.  If you can clearly hear the number being read out, call it good and move on for now; you can come back later if you want to improve.\n",
        "\n",
        "These artifacts are there in part because the dimensionality of the audio is small.  Take it as a matter of trust for now that if the audio signals are longer, the greater dimensionality of the data will make attacks imperceptible to a human listener.  We just can't fit that kind of rich data at-scale on a free service.  In the paper for the Carlini-Wagner (CW) attack (https://arxiv.org/pdf/1608.04644.pdf), you can see that the MNIST dataset also reveals artifacts for an attack that is well-tuned and more powerful than BIM.  But, CW is human imperceptible on CIFAR-10.  Tests on Audio MNIST are the same.\n",
        "\n",
        "Let's move on now to a defense.  You will be implementing a Density Estimation anomaly detection defense (https://arxiv.org/pdf/1703.00410v2.pdf).  The defense first constructs Gaussian mixture models (GMMs) on clean data to capture the distributions of that data.  Then it scores new data (possibly clean, possibly attacked) against those GMMs.\n",
        "\n",
        "Write some code in the block below to construct the densities.  Start with a simple Guassian distribution, and then experiment with tuning Guassian Mixture Models and Kernel Density functions.  There are useful library functions that will do the mathematical heavy lifting for you (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html, https://scikit-learn.org/stable/modules/density.html).  I recommend using the \"densities\" dictionary that is prepared for you, but you can manage your own data structures as you see fit.\n",
        "\n",
        "You'll need to add hooks to retrieve the activation outputs.  Experiment with different layer activations from the model to get the best results.\n",
        "\n",
        "Again, start by just constructing a dictionary of densities in this code block.  Once you have this, implement the density estimation scoring function in the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLSzzpSHTPYb"
      },
      "outputs": [],
      "source": [
        "input_limit = 3000 # tune this value if you have memory issues\n",
        "\n",
        "# Bring the training data back, with a batch size of 1\n",
        "train_loc = os.path.join(save_location, 'Preprocessed', 'Train')\n",
        "train_set = AudioDataset(train_loc)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 1, shuffle = True)\n",
        "\n",
        "activations = {}\n",
        "layer = 'fc1'\n",
        "\n",
        "# This function makes the activation values of a layer retrievable to us at runtime\n",
        "def get_activation(name):\n",
        "\n",
        "  def hook(model, input, output):\n",
        "\n",
        "    activations[name] = output.detach()\n",
        "\n",
        "  return hook\n",
        "\n",
        "# Register the hook here so we can get those activations\n",
        "net.fc1.register_forward_hook(get_activation('fc1'))\n",
        "\n",
        "densities = {}\n",
        "features_set = {}\n",
        "\n",
        "# Just preparing data structures\n",
        "for c in range(len(train_set.classes)):\n",
        "\n",
        "  densities[c] = []\n",
        "  features_set[c] = []\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for data, label in train_loader:\n",
        "\n",
        "'''\n",
        "\n",
        "Your code goes here\n",
        "\n",
        "'''\n",
        "\n",
        "if not os.path.isdir(os.path.join(save_location, 'Densities')):\n",
        "  os.mkdir(os.path.join(save_location, 'Densities'))\n",
        "\n",
        "# Comment this out if you need to save storage space\n",
        "pickle.dump(densities, open(os.path.join(save_location, 'Densities', 'densities.p'), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3b5FlWT9Z5V"
      },
      "source": [
        "This may be useful to recover a save point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVTD27NBfAzO"
      },
      "outputs": [],
      "source": [
        "densities = pickle.load(open(os.path.join(save_location, 'Densities', 'densities.p'), 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzSxmujR9f5T"
      },
      "source": [
        "Code your defense here.  Remember this takes data examples one-at-a-time, and returns a meaningful score for the new data against the density model.  Note that there is a built-in scoring function available for you to use (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.score_samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEOafUsGfcU3"
      },
      "outputs": [],
      "source": [
        "def density_estimation(densities, data):\n",
        "\n",
        "  # Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57DfaNPW9tZC"
      },
      "source": [
        "This code runs your defense for clean data, and then on your attacks!\n",
        "\n",
        "Note the negative sign at the call to the defense function.  This was natural for me - but, if your natural way of coding results in defense AUC scores that are very low (like .12), remove the negative signs!  .12 is a meaningful AUC score, because you can just flip the graph to get the much better score of .88.  An AUC score around .5 is where things are bad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRzzcetal3qu"
      },
      "outputs": [],
      "source": [
        "dev_loader = torch.utils.data.DataLoader(AudioDataset(dev_loc), batch_size = 1, shuffle = False)\n",
        "records = []\n",
        "\n",
        "for data, label in dev_loader:\n",
        "\n",
        "  data, label = data.to(device), label.to(device)\n",
        "\n",
        "  records.append((-density_estimation(densities, data.view(257, 196)), 'clean'))\n",
        "\n",
        "for attack in successful_attacks:\n",
        "  (data, orig_label, adv_label) = attack\n",
        "  data = torch.from_numpy(data).to(device)\n",
        "\n",
        "  records.append((-density_estimation(densities, data.view(257, 196)), 'attack'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5p9m5ik-ON5"
      },
      "source": [
        "Code up a function below to plot a ROC graph and calculated the ROC AUC.  If your AUC score is above .8, congratulations!   You have constructed a simple automated defense that can separate successful attacks from the clean data.\n",
        "\n",
        "This concludes the basic option for this project.  However, there are some natural next steps.  BIM attacks and Density Estimate anomaly detection are excellent for demonstrating the concepts, but they are not state-of-the-art.  Can you do better?  If you want to go further, try the CW attack (https://arxiv.org/pdf/1608.04644.pdf) and the ADA defense (https://arxiv.org/pdf/1712.06646.pdf)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Peu92cCrnYrf"
      },
      "outputs": [],
      "source": [
        "# ROC code goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Audio Adv ML Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+N1/SIDTV5F1Ej0Hz2Dq3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}